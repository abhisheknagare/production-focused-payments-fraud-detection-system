"""
Model Monitoring and Drift Detection
Tracks model performance degradation and triggers retraining
"""

import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from scipy import stats
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')


class ModelMonitor:
    """
    Monitor model performance and detect drift
    """

    def __init__(self, baseline_data: pd.DataFrame, production_data: pd.DataFrame):
        """
        Initialize monitor

        Args:
            baseline_data: Training data (baseline distribution)
            production_data: Recent production data
        """
        self.baseline = baseline_data.copy()
        self.production = production_data.copy()

        self.feature_cols = [col for col in baseline_data.columns if col.startswith('feat_')]

        print(f"Initialized monitor:")
        print(f"  Baseline: {len(self.baseline):,} transactions")
        print(f"  Production: {len(self.production):,} transactions")
        print(f"  Features: {len(self.feature_cols)}")

    def calculate_psi(self, baseline_array: np.ndarray, production_array: np.ndarray,
                     bins: int = 10) -> float:
        """
        Calculate Population Stability Index (PSI)

        PSI measures distribution shift between baseline and production
        PSI < 0.1: No significant change
        0.1 < PSI < 0.25: Moderate change - investigate
        PSI > 0.25: Significant change - retrain

        Args:
            baseline_array: Baseline feature values
            production_array: Production feature values
            bins: Number of bins for discretization

        Returns:
            PSI value
        """

        #Handle edge cases
        if len(baseline_array) == 0 or len(production_array) == 0:
            return 0.0

        #Create bins based on baseline distribution
        breakpoints = np.percentile(baseline_array, np.linspace(0, 100, bins + 1))
        breakpoints = np.unique(breakpoints)  #Remove duplicates

        if len(breakpoints) <= 1:
            return 0.0

        #Calculate distributions
        baseline_counts, _ = np.histogram(baseline_array, bins=breakpoints)
        production_counts, _ = np.histogram(production_array, bins=breakpoints)

        #Convert to percentages (avoid division by zero)
        baseline_percents = baseline_counts / len(baseline_array)
        production_percents = production_counts / len(production_array)

        #PSI calculation (add small epsilon to avoid log(0))
        epsilon = 1e-10
        baseline_percents = np.maximum(baseline_percents, epsilon)
        production_percents = np.maximum(production_percents, epsilon)

        psi = np.sum((production_percents - baseline_percents) *
                     np.log(production_percents / baseline_percents))

        return psi

    def detect_feature_drift(self) -> pd.DataFrame:
        """
        Detect drift for all features using PSI

        Returns:
            DataFrame with drift metrics per feature
        """


        print("\nFEATURE DRIFT DETECTION (PSI)")

        drift_results = []

        for feature in self.feature_cols:
            baseline_values = self.baseline[feature].dropna().values
            production_values = self.production[feature].dropna().values

            psi = self.calculate_psi(baseline_values, production_values)

            #Determine drift status
            if psi < 0.1:
                status = "Stable"
                action = "None"
            elif psi < 0.25:
                status = "Moderate Drift"
                action = "Investigate"
            else:
                status = "Significant Drift"
                action = "Retrain Required"

            drift_results.append({
                'feature': feature,
                'psi': psi,
                'status': status,
                'action': action,
                'baseline_mean': baseline_values.mean(),
                'production_mean': production_values.mean(),
                'baseline_std': baseline_values.std(),
                'production_std': production_values.std()
            })

        drift_df = pd.DataFrame(drift_results).sort_values('psi', ascending=False)

        #Summary
        high_drift = (drift_df['psi'] >= 0.25).sum()
        moderate_drift = ((drift_df['psi'] >= 0.1) & (drift_df['psi'] < 0.25)).sum()
        stable = (drift_df['psi'] < 0.1).sum()

        print(f"\nDrift Summary:")
        print(f"  High drift (PSI ≥ 0.25): {high_drift} features")
        print(f"  Moderate drift (0.1 ≤ PSI < 0.25): {moderate_drift} features")
        print(f"  Stable (PSI < 0.1): {stable} features")

        if high_drift > 0:
            print(f"\nACTION REQUIRED: {high_drift} features show significant drift")
            print("Recommendation: Retrain model with recent data")

        print(f"\nTop 10 Drifted Features:")
        print(drift_df[['feature', 'psi', 'status']].head(10).to_string(index=False))

        return drift_df

    def detect_score_drift(self, baseline_scores: np.ndarray,
                          production_scores: np.ndarray) -> Dict:
        """
        Detect drift in model score distribution

        Args:
            baseline_scores: Scores from training data
            production_scores: Scores from production

        Returns:
            Dictionary with drift metrics
        """


        print("\nSCORE DISTRIBUTION DRIFT")

        #Statistical tests
        ks_statistic, ks_pvalue = stats.ks_2samp(baseline_scores, production_scores)

        #Distribution metrics
        baseline_mean = baseline_scores.mean()
        production_mean = production_scores.mean()
        mean_shift = (production_mean - baseline_mean) / baseline_mean * 100

        baseline_std = baseline_scores.std()
        production_std = production_scores.std()

        #PSI for scores
        psi = self.calculate_psi(baseline_scores, production_scores, bins=20)

        #Determine drift status
        if psi < 0.1 and abs(mean_shift) < 10:
            status = "Stable"
            action = "Continue monitoring"
        elif psi < 0.25 or abs(mean_shift) < 20:
            status = "Moderate drift"
            action = "Investigate and consider retraining"
        else:
            status = "Significant drift"
            action = "Retrain immediately"

        results = {
            'psi': psi,
            'ks_statistic': ks_statistic,
            'ks_pvalue': ks_pvalue,
            'baseline_mean': baseline_mean,
            'production_mean': production_mean,
            'mean_shift_percent': mean_shift,
            'baseline_std': baseline_std,
            'production_std': production_std,
            'status': status,
            'action': action
        }

        print(f"\nScore Distribution Metrics:")
        print(f"  PSI: {psi:.4f}")
        print(f"  Mean shift: {mean_shift:+.2f}%")
        print(f"  Baseline mean: {baseline_mean:.4f}")
        print(f"  Production mean: {production_mean:.4f}")
        print(f"  KS statistic: {ks_statistic:.4f} (p={ks_pvalue:.4f})")
        print(f"\nStatus: {status}")
        print(f"Action: {action}")

        return results

    def monitor_business_metrics(self, production_predictions: pd.DataFrame) -> Dict:
        """
        Monitor business KPIs

        Args:
            production_predictions: DataFrame with actual_fraud and predicted_score

        Returns:
            Dictionary with business metrics
        """


        print("\nBUSINESS METRICS MONITORING")

        threshold = 0.950

        total_transactions = len(production_predictions)
        predicted_fraud = (production_predictions['predicted_score'] >= threshold).sum()
        approval_rate = (1 - predicted_fraud / total_transactions) * 100

        #If we have actual labels
        if 'actual_fraud' in production_predictions.columns:
            actual_fraud = production_predictions['actual_fraud'].sum()
            true_positives = ((production_predictions['predicted_score'] >= threshold) &
                            (production_predictions['actual_fraud'] == 1)).sum()
            false_positives = ((production_predictions['predicted_score'] >= threshold) &
                             (production_predictions['actual_fraud'] == 0)).sum()

            if actual_fraud > 0:
                fraud_detection_rate = (true_positives / actual_fraud) * 100
            else:
                fraud_detection_rate = 0.0

            if predicted_fraud > 0:
                precision = (true_positives / predicted_fraud) * 100
            else:
                precision = 0.0
        else:
            fraud_detection_rate = None
            precision = None
            actual_fraud = None

        metrics = {
            'total_transactions': total_transactions,
            'predicted_fraud_count': predicted_fraud,
            'predicted_fraud_rate': (predicted_fraud / total_transactions) * 100,
            'approval_rate': approval_rate,
            'fraud_detection_rate': fraud_detection_rate,
            'precision': precision,
            'actual_fraud_count': actual_fraud
        }

        print(f"\nOperational Metrics:")
        print(f"  Total transactions: {total_transactions:,}")
        print(f"  Predicted fraud: {predicted_fraud:,} ({metrics['predicted_fraud_rate']:.2f}%)")
        print(f"  Approval rate: {approval_rate:.2f}%")

        if fraud_detection_rate is not None:
            print(f"\nPerformance Metrics:")
            print(f"  Actual fraud: {actual_fraud:,}")
            print(f"  Fraud detection rate: {fraud_detection_rate:.2f}%")
            print(f"  Precision: {precision:.2f}%")

        #Alerts
        print("\nALERTS")

        if approval_rate < 95:
            print(f"ALERT: Approval rate {approval_rate:.1f}% below 95% threshold")

        if fraud_detection_rate and fraud_detection_rate < 75:
            print(f"ALERT: Fraud detection rate {fraud_detection_rate:.1f}% below 75% threshold")

        if precision and precision < 50:
            print(f"WARNING: Precision {precision:.1f}% below 50% threshold")

        if approval_rate >= 95 and (not fraud_detection_rate or fraud_detection_rate >= 75):
            print("All metrics within acceptable ranges")

        return metrics

    def plot_drift_analysis(self, drift_df: pd.DataFrame,
                           baseline_scores: np.ndarray,
                           production_scores: np.ndarray,
                           save_path: str = '../monitoring/reports/drift_analysis.png'):
        """
        Create comprehensive drift visualization
        """

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        #1. Feature PSI Chart
        top_features = drift_df.head(15)
        colors = ['red' if psi >= 0.25 else 'orange' if psi >= 0.1 else 'green'
                 for psi in top_features['psi']]

        axes[0, 0].barh(range(len(top_features)), top_features['psi'], color=colors)
        axes[0, 0].set_yticks(range(len(top_features)))
        axes[0, 0].set_yticklabels(top_features['feature'], fontsize=8)
        axes[0, 0].axvline(x=0.1, color='orange', linestyle='--', linewidth=2, label='Moderate (0.1)')
        axes[0, 0].axvline(x=0.25, color='red', linestyle='--', linewidth=2, label='High (0.25)')
        axes[0, 0].set_xlabel('PSI Value', fontweight='bold')
        axes[0, 0].set_title('Top 15 Feature Drift (PSI)', fontweight='bold', fontsize=12)
        axes[0, 0].legend()
        axes[0, 0].invert_yaxis()

        #2. Score Distribution Comparison
        axes[0, 1].hist([baseline_scores, production_scores], bins=50,
                       label=['Baseline (Training)', 'Production'],
                       alpha=0.7, color=['blue', 'red'])
        axes[0, 1].axvline(baseline_scores.mean(), color='blue', linestyle='--',
                          linewidth=2, label=f'Baseline Mean: {baseline_scores.mean():.3f}')
        axes[0, 1].axvline(production_scores.mean(), color='red', linestyle='--',
                          linewidth=2, label=f'Production Mean: {production_scores.mean():.3f}')
        axes[0, 1].set_xlabel('Fraud Score', fontweight='bold')
        axes[0, 1].set_ylabel('Frequency', fontweight='bold')
        axes[0, 1].set_title('Score Distribution Drift', fontweight='bold', fontsize=12)
        axes[0, 1].legend()

        #3. Drift Category Breakdown
        drift_categories = drift_df['status'].value_counts()
        axes[1, 0].pie(drift_categories.values, labels=drift_categories.index,
                      autopct='%1.1f%%', startangle=90,
                      colors=['green', 'orange', 'red'])
        axes[1, 0].set_title('Feature Stability Distribution', fontweight='bold', fontsize=12)

        #4. Mean Shift Analysis
        top_shifted = drift_df.nlargest(10, 'psi')
        mean_shifts = ((top_shifted['production_mean'] - top_shifted['baseline_mean']) /
                      (top_shifted['baseline_mean'].abs() + 1e-10) * 100)

        axes[1, 1].barh(range(len(top_shifted)), mean_shifts,
                       color=['red' if x > 20 else 'orange' if abs(x) > 10 else 'green'
                             for x in mean_shifts])
        axes[1, 1].set_yticks(range(len(top_shifted)))
        axes[1, 1].set_yticklabels(top_shifted['feature'], fontsize=8)
        axes[1, 1].axvline(x=0, color='black', linewidth=1)
        axes[1, 1].axvline(x=-20, color='red', linestyle='--', alpha=0.5)
        axes[1, 1].axvline(x=20, color='red', linestyle='--', alpha=0.5)
        axes[1, 1].set_xlabel('Mean Shift (%)', fontweight='bold')
        axes[1, 1].set_title('Feature Mean Shift Analysis', fontweight='bold', fontsize=12)
        axes[1, 1].invert_yaxis()

        plt.suptitle('Model Drift Analysis Dashboard', fontsize=16, fontweight='bold', y=0.995)
        plt.tight_layout()

        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nDrift analysis saved to: {save_path}")
        plt.close()

    def should_retrain(self, drift_df: pd.DataFrame, score_drift: Dict,
                      business_metrics: Dict) -> Tuple[bool, List[str]]:
        """
        Determine if model should be retrained

        Returns:
            (should_retrain, reasons)
        """

        print("\nRETRAIN DECISION")

        reasons = []

        #Check 1: Feature drift
        high_drift_features = (drift_df['psi'] >= 0.25).sum()
        if high_drift_features >= 3:
            reasons.append(f"High drift in {high_drift_features} features (threshold: 3)")

        #Check 2: Score drift
        if score_drift['psi'] >= 0.25:
            reasons.append(f"Score PSI {score_drift['psi']:.3f} exceeds 0.25")

        #Check 3: Mean shift
        if abs(score_drift['mean_shift_percent']) > 20:
            reasons.append(f"Score mean shifted {score_drift['mean_shift_percent']:+.1f}% (threshold: ±20%)")

        #Check 4: Business metrics
        if business_metrics['approval_rate'] < 95:
            reasons.append(f"Approval rate {business_metrics['approval_rate']:.1f}% below 95%")

        if business_metrics['fraud_detection_rate'] and business_metrics['fraud_detection_rate'] < 75:
            reasons.append(f"Fraud detection rate {business_metrics['fraud_detection_rate']:.1f}% below 75%")

        should_retrain = len(reasons) > 0

        print(f"\nRetrain Decision: {'YES' if should_retrain else 'NO'}")

        if should_retrain:
            print(f"\nReasons ({len(reasons)}):")
            for i, reason in enumerate(reasons, 1):
                print(f"  {i}. {reason}")
        else:
            print("\nAll metrics within acceptable ranges. Continue monitoring.")

        return should_retrain, reasons

def main():
    """Main monitoring pipeline"""

    print("\nMODEL MONITORING & DRIFT DETECTION")

    #Load baseline (training) data
    print("\nLoading baseline data (training set)...")

    df_all = pd.read_csv('../data/processed/transactions_with_features.csv')


    df_all['timestamp'] = pd.to_datetime(df_all['timestamp'])
    df_all = df_all.sort_values('timestamp')

    #Split: First 75% = baseline, Last 25% = production
    split_idx = int(len(df_all) * 0.75)
    baseline_data = df_all.iloc[:split_idx].copy()
    production_data = df_all.iloc[split_idx:].copy()

    print(f"Baseline period: {baseline_data['timestamp'].min().date()} to {baseline_data['timestamp'].max().date()}")
    print(f"Production period: {production_data['timestamp'].min().date()} to {production_data['timestamp'].max().date()}")

    #Load model and get scores
    print("\nLoading model and generating scores...")
    with open('../models/trained/lightgbm_production.pkl', 'rb') as f:
        model = pickle.load(f)

    feature_cols = [col for col in baseline_data.columns if col.startswith('feat_')]

    X_baseline = baseline_data[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)
    X_production = production_data[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)

    baseline_scores = model.predict(X_baseline)
    production_scores = model.predict(X_production)

    monitor = ModelMonitor(baseline_data, production_data)

    #1. Detect feature drift
    drift_df = monitor.detect_feature_drift()

    #2. Detect score drift
    score_drift = monitor.detect_score_drift(baseline_scores, production_scores)

    #3. Monitor business metrics
    production_predictions = pd.DataFrame({
        'actual_fraud': production_data['is_fraud'].values,
        'predicted_score': production_scores
    })
    business_metrics = monitor.monitor_business_metrics(production_predictions)

    #4. Visualize drift
    monitor.plot_drift_analysis(drift_df, baseline_scores, production_scores)

    #5. Retrain decision
    should_retrain, reasons = monitor.should_retrain(drift_df, score_drift, business_metrics)

    #6. Save results
    drift_df.to_csv('../monitoring/reports/drift_report.csv', index=False)

    with open('../monitoring/reports/monitoring_summary.txt', 'w') as f:
        f.write("\nMODEL MONITORING SUMMARY\n")
        f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"Retrain Required: {'YES' if should_retrain else 'NO'}\n\n")
        if should_retrain:
            f.write("Reasons:\n")
            for reason in reasons:
                f.write(f"  - {reason}\n")
        f.write(f"\nScore Drift PSI: {score_drift['psi']:.4f}\n")
        f.write(f"Approval Rate: {business_metrics['approval_rate']:.2f}%\n")
        if business_metrics['fraud_detection_rate']:
            f.write(f"Fraud Detection Rate: {business_metrics['fraud_detection_rate']:.2f}%\n")

    print("\nMONITORING COMPLETE")
    print("\nReports saved:")
    print("  - monitoring/reports/drift_report.csv")
    print("  - monitoring/reports/drift_analysis.png")
    print("  - monitoring/reports/monitoring_summary.txt")


if __name__ == "__main__":
    main()
